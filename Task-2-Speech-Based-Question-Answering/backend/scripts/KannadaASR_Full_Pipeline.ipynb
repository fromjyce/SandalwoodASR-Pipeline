{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WOdEs4UZsyLJ"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "zip_path = '/content/drive/MyDrive/ColabDrive/IIITB/OneDrive_71_10-30-2024.zip'\n",
    "extract_dir = '/content/dataset'\n",
    "with ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zk4W6uqvs2PR",
    "outputId": "9c6beedf-074d-4b7a-ba12-30aae8c4f7de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dir_path = '/content/dataset'\n",
    "files = os.listdir(dir_path)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7sru-BT-pqPk",
    "outputId": "928f712a-ac9f-4369-96e9-8d8ae59ffcec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 12 06:10:46 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   44C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info  = !nvidia-smi\n",
    "gpu_info = \"\\n\".join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZjy2dABzRRj"
   },
   "source": [
    "## Sound Noise Reducting Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0Ipwc3VZzC4e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython import display as disp\n",
    "from torchaudio.transforms import Resample\n",
    "#from denoiser.dsp import convert_audio\n",
    "#from denoiser import pretrained\n",
    "import soundfile\n",
    "#from pystoi import stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xy2N4JGMzT2L",
    "outputId": "971888e4-269e-49ed-df0d-28d12202f98a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pip3', 'install', 'pystoi'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"pip3\", \"install\", \"IPython\"])\n",
    "subprocess.run([\"pip3\", \"install\", \"glob2\"])\n",
    "subprocess.run([\"pip3\", \"install\", \"soundfile\"])\n",
    "subprocess.run([\"pip3\", \"install\", \"torchaudio\", \"--upgrade\"])\n",
    "subprocess.run([\"pip3\", \"install\", \"transformers\", \"--upgrade\"])\n",
    "subprocess.run([\"pip3\", \"install\", \"pystoi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a0fGmfX0g0z"
   },
   "source": [
    "### Denoiser Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZ0BcNlB0i9p"
   },
   "outputs": [],
   "source": [
    "#pretrained.py\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "# author: adefossez\n",
    "\n",
    "import logging\n",
    "\n",
    "import torch.hub\n",
    "\n",
    "from .demucs import Demucs\n",
    "from .utils import deserialize_model\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "ROOT = \"https://dl.fbaipublicfiles.com/adiyoss/denoiser/\"\n",
    "DNS_48_URL = ROOT + \"dns48-11decc9d8e3f0998.th\"\n",
    "DNS_64_URL = ROOT + \"dns64-a7761ff99a7d5bb6.th\"\n",
    "MASTER_64_URL = ROOT + \"master64-8a5dfb4bb92753dd.th\"\n",
    "VALENTINI_NC = ROOT + 'valentini_nc-93fc4337.th'  # Non causal Demucs on Valentini\n",
    "\n",
    "\n",
    "def _demucs(pretrained, url, **kwargs):\n",
    "    model = Demucs(**kwargs, sample_rate=16_000)\n",
    "    if pretrained:\n",
    "        state_dict = torch.hub.load_state_dict_from_url(url, map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def dns48(pretrained=True):\n",
    "    return _demucs(pretrained, DNS_48_URL, hidden=48)\n",
    "\n",
    "\n",
    "def dns64(pretrained=True):\n",
    "    return _demucs(pretrained, DNS_64_URL, hidden=64)\n",
    "\n",
    "\n",
    "def master64(pretrained=True):\n",
    "    return _demucs(pretrained, MASTER_64_URL, hidden=64)\n",
    "\n",
    "\n",
    "def valentini_nc(pretrained=True):\n",
    "    return _demucs(pretrained, VALENTINI_NC, hidden=64, causal=False, stride=2, resample=2)\n",
    "\n",
    "\n",
    "def add_model_flags(parser):\n",
    "    group = parser.add_mutually_exclusive_group(required=False)\n",
    "    group.add_argument(\"-m\", \"--model_path\", help=\"Path to local trained model.\")\n",
    "    group.add_argument(\"--dns48\", action=\"store_true\",\n",
    "                       help=\"Use pre-trained real time H=48 model trained on DNS.\")\n",
    "    group.add_argument(\"--dns64\", action=\"store_true\",\n",
    "                       help=\"Use pre-trained real time H=64 model trained on DNS.\")\n",
    "    group.add_argument(\"--master64\", action=\"store_true\",\n",
    "                       help=\"Use pre-trained real time H=64 model trained on DNS and Valentini.\")\n",
    "    group.add_argument(\"--valentini_nc\", action=\"store_true\",\n",
    "                       help=\"Use pre-trained H=64 model trained on Valentini, non causal.\")\n",
    "\n",
    "\n",
    "def get_model(args):\n",
    "    \"\"\"\n",
    "    Load local model package or torchhub pre-trained model.\n",
    "    \"\"\"\n",
    "    if args.model_path:\n",
    "        logger.info(\"Loading model from %s\", args.model_path)\n",
    "        pkg = torch.load(args.model_path, 'cpu')\n",
    "        if 'model' in pkg:\n",
    "            if 'best_state' in pkg:\n",
    "                pkg['model']['state'] = pkg['best_state']\n",
    "            model = deserialize_model(pkg['model'])\n",
    "        else:\n",
    "            model = deserialize_model(pkg)\n",
    "    elif args.dns64:\n",
    "        logger.info(\"Loading pre-trained real time H=64 model trained on DNS.\")\n",
    "        model = dns64()\n",
    "    elif args.master64:\n",
    "        logger.info(\"Loading pre-trained real time H=64 model trained on DNS and Valentini.\")\n",
    "        model = master64()\n",
    "    elif args.valentini_nc:\n",
    "        logger.info(\"Loading pre-trained H=64 model trained on Valentini.\")\n",
    "        model = valentini_nc()\n",
    "    else:\n",
    "        logger.info(\"Loading pre-trained real time H=48 model trained on DNS.\")\n",
    "        model = dns48()\n",
    "    logger.debug(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4m0hei10stI"
   },
   "outputs": [],
   "source": [
    "### dsp.py\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "# author: adefossez\n",
    "\n",
    "import julius\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def hz_to_mel(f):\n",
    "    return 2595 * np.log10(1 + f / 700)\n",
    "\n",
    "\n",
    "def mel_to_hz(m):\n",
    "    return 700 * (10**(m / 2595) - 1)\n",
    "\n",
    "\n",
    "def mel_frequencies(n_mels, fmin, fmax):\n",
    "    low = hz_to_mel(fmin)\n",
    "    high = hz_to_mel(fmax)\n",
    "    mels = np.linspace(low, high, n_mels)\n",
    "    return mel_to_hz(mels)\n",
    "\n",
    "\n",
    "def convert_audio_channels(wav, channels=2):\n",
    "    \"\"\"Convert audio to the given number of channels.\"\"\"\n",
    "    *shape, src_channels, length = wav.shape\n",
    "    if src_channels == channels:\n",
    "        pass\n",
    "    elif channels == 1:\n",
    "        # Case 1:\n",
    "        # The caller asked 1-channel audio, but the stream have multiple\n",
    "        # channels, downmix all channels.\n",
    "        wav = wav.mean(dim=-2, keepdim=True)\n",
    "    elif src_channels == 1:\n",
    "        # Case 2:\n",
    "        # The caller asked for multiple channels, but the input file have\n",
    "        # one single channel, replicate the audio over all channels.\n",
    "        wav = wav.expand(*shape, channels, length)\n",
    "    elif src_channels >= channels:\n",
    "        # Case 3:\n",
    "        # The caller asked for multiple channels, and the input file have\n",
    "        # more channels than requested. In that case return the first channels.\n",
    "        wav = wav[..., :channels, :]\n",
    "    else:\n",
    "        # Case 4: What is a reasonable choice here?\n",
    "        raise ValueError('The audio file has less channels than requested but is not mono.')\n",
    "    return wav\n",
    "\n",
    "\n",
    "def convert_audio(wav, from_samplerate, to_samplerate, channels):\n",
    "    \"\"\"Convert audio from a given samplerate to a target one and target number of channels.\"\"\"\n",
    "    wav = convert_audio_channels(wav, channels)\n",
    "    return julius.resample_frac(wav, from_samplerate, to_samplerate)\n",
    "\n",
    "\n",
    "class LowPassFilters(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Bank of low pass filters.\n",
    "\n",
    "    Args:\n",
    "        cutoffs (list[float]): list of cutoff frequencies, in [0, 1] expressed as `f/f_s` where\n",
    "            f_s is the samplerate.\n",
    "        width (int): width of the filters (i.e. kernel_size=2 * width + 1).\n",
    "            Default to `2 / min(cutoffs)`. Longer filters will have better attenuation\n",
    "            but more side effects.\n",
    "    Shape:\n",
    "        - Input: `(*, T)`\n",
    "        - Output: `(F, *, T` with `F` the len of `cutoffs`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cutoffs: list, width: int = None):\n",
    "        super().__init__()\n",
    "        self.cutoffs = cutoffs\n",
    "        if width is None:\n",
    "            width = int(2 / min(cutoffs))\n",
    "        self.width = width\n",
    "        window = torch.hamming_window(2 * width + 1, periodic=False)\n",
    "        t = np.arange(-width, width + 1, dtype=np.float32)\n",
    "        filters = []\n",
    "        for cutoff in cutoffs:\n",
    "            sinc = torch.from_numpy(np.sinc(2 * cutoff * t))\n",
    "            filters.append(2 * cutoff * sinc * window)\n",
    "        self.register_buffer(\"filters\", torch.stack(filters).unsqueeze(1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        *others, t = input.shape\n",
    "        input = input.view(-1, 1, t)\n",
    "        out = F.conv1d(input, self.filters, padding=self.width)\n",
    "        return out.permute(1, 0, 2).reshape(-1, *others, t)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"LossPassFilters(width={},cutoffs={})\".format(self.width, self.cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukfIaqvb_ot5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython import display as disp\n",
    "from torchaudio.transforms import Resample\n",
    "from denoiser.dsp import convert_audio\n",
    "from denoiser import pretrained\n",
    "import soundfile\n",
    "from pystoi import stoi\n",
    "\n",
    "# Installations\n",
    "######################################################################\n",
    "subprocess.run([\"pip3\", \"install\", \"IPython\"])\n",
    "subprocess.run([\"pip3\", \"install\", \"glob2\"])\n",
    "subprocess.run([\"pip3\", \"install\", \"soundfile\"])\n",
    "subprocess.run([\"pip3\", \"install\", \"torchaudio\", \"--upgrade\"])\n",
    "subprocess.run([\"pip3\", \"install\", \"transformers\", \"--upgrade\"])\n",
    "subprocess.run([\"pip3\", \"install\", \"pystoi\"])\n",
    "######################################################################\n",
    "\n",
    "def dns_64(input_file, output_dir):\n",
    "    LIST_OF_AUDIO_FILES = glob.glob(input_file)\n",
    "    model = pretrained.dns64()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    for idx, audio_file in enumerate(LIST_OF_AUDIO_FILES):\n",
    "        try:\n",
    "            wav, sr = torchaudio.load(audio_file)\n",
    "            wav = convert_audio(wav, sr, model.sample_rate, model.chin)\n",
    "            wav = wav.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                denoised = model(wav[None])[0]\n",
    "                denoised_cpu = denoised.cpu()\n",
    "\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                output_file = os.path.join(output_dir, f\"denoised_{idx + 1}.mp3\")\n",
    "                torchaudio.save(output_file, denoised_cpu, model.sample_rate)\n",
    "\n",
    "                # Compute STOI score\n",
    "                denoised_np = denoised_cpu.numpy()[0]\n",
    "                wav_np = wav.cpu().numpy()[0]\n",
    "                stoi_score = stoi(wav_np, denoised_np, model.sample_rate, extended=False)\n",
    "                print(f\"STOI Score for {audio_file}: {stoi_score}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_file}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r\"./INPUT/AUDIO/audio*.mp3\"\n",
    "    output_dir = \"./DENOISED_OUTPUT\"\n",
    "    dns_64(input_file, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7A_o7JU_563"
   },
   "source": [
    "## Voice Improvement Engine and Speech Isolation Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mz4Dcpzu_78F"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "# Define base paths\n",
    "BASE_PATH = \".\"\n",
    "INPUT_MP3S = os.path.join(BASE_PATH, \"INPUT_MP3S\")\n",
    "OUTPUT_PATH = os.path.join(BASE_PATH, \"OUTPUT\")\n",
    "INTERMITTENT_CSVS_WITH_AUDIO_CUT_TIME = os.path.join(OUTPUT_PATH, \"TEXT\")\n",
    "INTERMITTENT_CUT_AUDIOS = os.path.join(OUTPUT_PATH, \"AUDIO\")\n",
    "\n",
    "# Ensure necessary directories exist\n",
    "def setup_directories():\n",
    "    delete_path(INTERMITTENT_CSVS_WITH_AUDIO_CUT_TIME)\n",
    "    delete_path(INTERMITTENT_CUT_AUDIOS)\n",
    "    os.makedirs(INPUT_MP3S, exist_ok=True)\n",
    "    os.makedirs(INTERMITTENT_CSVS_WITH_AUDIO_CUT_TIME, exist_ok=True)\n",
    "    os.makedirs(INTERMITTENT_CUT_AUDIOS, exist_ok=True)\n",
    "\n",
    "# Run speaker diarization and save results to CSV\n",
    "def run_pyannote_seg_3(input_file, output_csv):\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=\"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\"\n",
    "    )\n",
    "    pipeline.to(torch.device(\"cpu\"))\n",
    "\n",
    "    diarization = pipeline(input_file)\n",
    "    with open(output_csv, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['speaker', 'start_time', 'stop_time'])\n",
    "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "            csv_writer.writerow([f'speaker_{speaker}', f'{turn.start:.1f}', f'{turn.end:.1f}'])\n",
    "\n",
    "# Process audio file for segmentation\n",
    "def process_audio(input_mp3s_directory, mp3_file_name):\n",
    "    mp3_file = os.path.join(input_mp3s_directory, mp3_file_name)\n",
    "    print(f\"Now, Working On  --->  {mp3_file}\")\n",
    "\n",
    "    tmp_csv = os.path.join(INTERMITTENT_CSVS_WITH_AUDIO_CUT_TIME, f'{mp3_file_name}_MODEL_ANALYSIS.csv')\n",
    "    run_pyannote_seg_3(mp3_file, tmp_csv)\n",
    "\n",
    "    segment_number = 0\n",
    "    output_directory = INTERMITTENT_CUT_AUDIOS\n",
    "    NEW_DIR = os.path.join(output_directory, f\"{mp3_file_name}_CUTS\")\n",
    "    os.makedirs(NEW_DIR, exist_ok=True)\n",
    "\n",
    "    with open(tmp_csv, 'r') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        next(csv_reader, None)\n",
    "\n",
    "        for row in csv_reader:\n",
    "            speaker, start_time, stop_time = row\n",
    "            start_time_str, stop_time_str = str(start_time), str(stop_time)\n",
    "            output_audio = f\"CUT_{mp3_file_name}__{segment_number}.mp3\"\n",
    "            output_path = os.path.join(output_directory, output_audio)\n",
    "\n",
    "            command = [\n",
    "                'ffmpeg', '-y', '-i', mp3_file,\n",
    "                '-ss', start_time_str, '-to', stop_time_str,\n",
    "                '-c:a', 'libmp3lame', '-q:a', '0',\n",
    "                '-vn', '-c', 'copy', output_path\n",
    "            ]\n",
    "            subprocess.run(command)\n",
    "            segment_number += 1\n",
    "\n",
    "    move_file(tmp_csv, os.path.join(INTERMITTENT_CSVS_WITH_AUDIO_CUT_TIME, f'{mp3_file_name}_MODEL_ANALYSIS.csv'))\n",
    "    move_and_rename_mp3_files(output_directory, NEW_DIR)\n",
    "\n",
    "# Utility functions\n",
    "def delete_path(directory_path):\n",
    "    try:\n",
    "        if os.path.exists(directory_path):\n",
    "            shutil.rmtree(directory_path)\n",
    "            print(f\"Directory deleted: {directory_path}\")\n",
    "        else:\n",
    "            print(f\"Directory not found: {directory_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "def force_delete_file(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting file '{file_path}': {e}\")\n",
    "\n",
    "def get_mp3_files(directory):\n",
    "    return [file for file in os.listdir(directory) if file.endswith(\".mp3\")]\n",
    "\n",
    "def move_file(src, dest):\n",
    "    try:\n",
    "        shutil.move(src, dest)\n",
    "    except Exception as e:\n",
    "        print(f\"Error moving file from '{src}' to '{dest}': {e}\")\n",
    "\n",
    "def move_and_rename_mp3_files(input_directory, output_directory):\n",
    "    mp3_files = get_mp3_files(input_directory)\n",
    "    for mp3_file in mp3_files:\n",
    "        input_path = os.path.join(input_directory, mp3_file)\n",
    "        output_path = os.path.join(output_directory, mp3_file)\n",
    "        os.rename(input_path, output_path)\n",
    "        print(f\"Moved and renamed: {mp3_file} -> {output_path}\")\n",
    "\n",
    "# Main script to process all MP3 files\n",
    "if __name__ == \"__main__\":\n",
    "    setup_directories()\n",
    "    mp3_files = get_mp3_files(INPUT_MP3S)\n",
    "    for mp3_file in mp3_files:\n",
    "        process_audio(INPUT_MP3S, mp3_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kannada Audio Transcription System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huoKUT1CAAEH"
   },
   "source": [
    "### Adding ASR - W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ova4REED__vl"
   },
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "model_id = \"addy88/wav2vec2-kannada-stt\"\n",
    "\n",
    "audio_dir = \"./OUTPUT/AUDIO/1.mp3_CUTS\"\n",
    "# audio_dir = \"./INPUT_MP3S\"\n",
    "output_dir = \"./OUTPUT/TEXT/01a_Transcription\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List all .mp3 files in the directory\n",
    "audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.mp3')]\n",
    "\n",
    "# Load the processor for inference\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
    "\n",
    "def process_audio_file(i, audio_file):\n",
    "    audio_path = os.path.join(audio_dir, audio_file)\n",
    "\n",
    "    try:\n",
    "        audio, orig_freq = torchaudio.load(audio_path)\n",
    "\n",
    "        # Ensure mono channel:\n",
    "        audio = audio.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Resample to model-compatible frequency (check documentation):\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq, 16_000)  # assuming 16 kHz\n",
    "        audio = resampler(audio)\n",
    "\n",
    "        # Prepare audio input for the model:\n",
    "        audio_input = {\"input_values\": audio}\n",
    "\n",
    "        inputs = processor(\n",
    "            audio_input[\"input_values\"].numpy(),\n",
    "            return_tensors=\"pt\",\n",
    "            sampling_rate=16_000\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs.input_values).logits\n",
    "\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "        # Remove <s> tokens from the transcription\n",
    "        cleaned_transcription = transcription.replace(\"<s>\", \"\").strip()\n",
    "\n",
    "        # Save transcription to a file\n",
    "        output_file = os.path.join(output_dir, f\"{i+1:02d}a_transcription.txt\")\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(cleaned_transcription)\n",
    "\n",
    "        print(f\"Transcription for {audio_file} saved to: {output_file}\")\n",
    "\n",
    "    except OSError as e:\n",
    "        print(f\"Error loading audio file {audio_file}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference for {audio_file}: {e}\")\n",
    "\n",
    "# Use ThreadPoolExecutor to process files in parallel\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_audio_file, i, audio_file) for i, audio_file in enumerate(audio_files)]\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        future.result()  # Retrieve the result to catch any exceptions\n",
    "\n",
    "print(\"All transcriptions are done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2L36IlzA8EN"
   },
   "source": [
    "### Adding ASR - Whispher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCJihRZrBMAv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "# Define input and output directories\n",
    "audio_dir = \"./OUTPUT/AUDIO/1.mp3_CUTS\"\n",
    "output_dir = \"./OUTPUT/TEXT/01a_Transcription\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List all .mp3 files in the directory\n",
    "audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.mp3')]\n",
    "\n",
    "# Set the device to CUDA if available, otherwise use CPU\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "try:\n",
    "    # Load the Whisper model for Kannada transcription\n",
    "    transcribe = pipeline(\n",
    "        task=\"automatic-speech-recognition\",\n",
    "        model=\"vasista22/whisper-kannada-tiny\",\n",
    "        chunk_length_s=15,\n",
    "        stride_length_s=5,\n",
    "        batch_size=1,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Set forced decoder to ensure Kannada language transcription\n",
    "    transcribe.model.config.forced_decoder_ids = transcribe.tokenizer.get_decoder_prompt_ids(language=\"kn\", task=\"transcribe\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Process each audio file\n",
    "for i, audio_file in enumerate(audio_files):\n",
    "    audio_path = os.path.join(audio_dir, audio_file)\n",
    "\n",
    "    try:\n",
    "        # Transcribe the audio file\n",
    "        transcription = transcribe(audio_path)[\"text\"]\n",
    "\n",
    "        # Save transcription to a text file\n",
    "        output_file = os.path.join(output_dir, f\"{i+1:02d}a_transcription.txt\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(transcription)\n",
    "\n",
    "        print(f\"Transcription for {audio_file} saved to: {output_file}\")\n",
    "\n",
    "    except OSError as e:\n",
    "        print(f\"Error loading audio file {audio_file}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription for {audio_file}: {e}\")\n",
    "\n",
    "print(\"All transcriptions completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nNUn2lIBRpj"
   },
   "source": [
    "### Adding ASR - Wav2Vec 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7aty_2IBfAU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "# Load the pre-trained model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"amoghsgopadi/wav2vec2-large-xlsr-kn\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"amoghsgopadi/wav2vec2-large-xlsr-kn\")\n",
    "\n",
    "# Load and preprocess your audio file\n",
    "def preprocess_audio(file_path):\n",
    "    # Load audio and resample it to 16 kHz\n",
    "    speech_array, sampling_rate = torchaudio.load(file_path)\n",
    "    resampler = torchaudio.transforms.Resample(sampling_rate, 16_000)\n",
    "    speech = resampler(speech_array).squeeze()  # remove extra dimensions\n",
    "    return speech\n",
    "\n",
    "# Function to transcribe audio and save the output to a text file\n",
    "def transcribe_and_save(audio_path, output_path):\n",
    "    # Preprocess the audio to get the speech tensor\n",
    "    speech = preprocess_audio(audio_path)\n",
    "\n",
    "    # Prepare the inputs for the model using the processor\n",
    "    inputs = processor(speech, sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Perform inference with the model\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n",
    "\n",
    "    # Get the predicted tokens from the logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Decode the tokens into text\n",
    "    transcription = processor.batch_decode(predicted_ids)\n",
    "\n",
    "    # Save the transcription to the output file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(transcription[0])  # transcriptions is a list\n",
    "    print(f\"Transcription saved to: {output_path}\")\n",
    "\n",
    "# Paths to your directories\n",
    "audio_dir = \"./OUTPUT/AUDIO/1.mp3_CUTS\"\n",
    "output_dir = \"./OUTPUT/TEXT/01a_Transcription\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List all .mp3 files in the directory\n",
    "audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.mp3')]\n",
    "\n",
    "# Process each audio file\n",
    "for i, audio_file in enumerate(audio_files):\n",
    "    audio_path = os.path.join(audio_dir, audio_file)\n",
    "    output_file = os.path.join(output_dir, f\"{i+1:02d}a_transcription.txt\")\n",
    "\n",
    "    try:\n",
    "        # Transcribe and save the output\n",
    "        transcribe_and_save(audio_path, output_file)\n",
    "    except OSError as e:\n",
    "        print(f\"Error loading audio file {audio_file}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference for {audio_file}: {e}\")\n",
    "\n",
    "print(\"All transcriptions are done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYLYrW15BjNq"
   },
   "source": [
    "## Punctuation Processing System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqSOF7_TBmUV"
   },
   "outputs": [],
   "source": [
    "from punctuators.models import PunctCapSegModelONNX\n",
    "from typing import List\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Instantiate the model\n",
    "m = PunctCapSegModelONNX.from_pretrained(\"pcs_47lang\")\n",
    "\n",
    "# Define input directory path\n",
    "input_dir = \"./OUTPUT/TEXT/01a_Transcription\"\n",
    "\n",
    "# List all .txt files ending with transcription.txt in the directory\n",
    "transcription_files = [f for f in os.listdir(input_dir) if f.endswith(\"transcription.txt\")]\n",
    "\n",
    "# Define output directory for punctuation files\n",
    "output_dir = \"./OUTPUT/TEXT/02_Punctuation\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def process_transcription_file(transcription_file):\n",
    "    input_file = os.path.join(input_dir, transcription_file)\n",
    "\n",
    "    try:\n",
    "        # Read input texts from the file\n",
    "        input_texts: List[str] = []\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                input_texts.append(line.strip())  # Remove leading/trailing whitespaces\n",
    "\n",
    "        # Run inference\n",
    "        results: List[List[str]] = m.infer(input_texts)\n",
    "\n",
    "        # Generate output file name\n",
    "        output_file_name = transcription_file.replace(\"transcription.txt\", \"punctuation.txt\")\n",
    "        output_file = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "        # Write results to file\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for output_texts in results:\n",
    "                for text in output_texts:\n",
    "                    f.write(f\"{text}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "        print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {transcription_file}: {e}\")\n",
    "\n",
    "# Use ThreadPoolExecutor to process files in parallel\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_transcription_file, transcription_file) for transcription_file in transcription_files]\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        future.result()  # Retrieve the result to catch any exceptions\n",
    "\n",
    "print(\"All punctuations are done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mcysANYCHKt"
   },
   "source": [
    "## Transliteration Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHhAthZMCLTc"
   },
   "outputs": [],
   "source": [
    "from om_transliterator import Transliterator\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Initialize the transliterator\n",
    "transliterator = Transliterator()\n",
    "\n",
    "# Define input directory path for punctuation files\n",
    "input_dir = \"./OUTPUT/TEXT/02_Punctuation\"\n",
    "\n",
    "# List all .txt files ending with punctuation.txt in the directory\n",
    "punctuation_files = [f for f in os.listdir(input_dir) if f.endswith(\"punctuation.txt\")]\n",
    "\n",
    "# Define output directory for transliteration files\n",
    "output_dir = \"./OUTPUT/TEXT/03_Transliteration\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def process_punctuation_file(punctuation_file):\n",
    "    input_file_path = os.path.join(input_dir, punctuation_file)\n",
    "    output_file_path = os.path.join(output_dir, punctuation_file.replace(\"punctuation.txt\", \"transliteration.txt\"))\n",
    "\n",
    "    try:\n",
    "        # Read the original text from the file\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Concatenate the lines into a single text string\n",
    "        original_text = \" \".join([line.strip() for line in lines])\n",
    "\n",
    "        # Perform transliteration\n",
    "        transliterated_text = transliterator.knda_to_latn(original_text.strip())\n",
    "\n",
    "        # Write the transliterated text to the output file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(transliterated_text)\n",
    "\n",
    "        print(f\"Transliteration results saved to: {output_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {punctuation_file}: {e}\")\n",
    "\n",
    "# Use ThreadPoolExecutor to process files in parallel\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_punctuation_file, punctuation_file) for punctuation_file in punctuation_files]\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        future.result()  # Retrieve the result to catch any exceptions\n",
    "\n",
    "print(\"All transliterations are done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ilSCuQ4Cazx"
   },
   "source": [
    "## Language Translation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAuvnhTVCc6j"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from IndicTransTokenizer import IndicProcessor\n",
    "\n",
    "# Initialize the transliterator\n",
    "ip = IndicProcessor(inference=True)\n",
    "\n",
    "# Model and tokenizer initialization\n",
    "model_name = \"ai4bharat/indictrans2-indic-en-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Define input directory path for punctuation files\n",
    "input_dir = \"./OUTPUT/TEXT/02_Punctuation\"\n",
    "\n",
    "# List all .txt files ending with punctuation.txt in the directory\n",
    "punctuation_files = [f for f in os.listdir(input_dir) if f.endswith(\"punctuation.txt\")]\n",
    "\n",
    "# Define output directory for translation files\n",
    "output_dir = \"./OUTPUT/TEXT/04_Translation\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each punctuation file\n",
    "for punctuation_file in punctuation_files:\n",
    "    input_file = os.path.join(input_dir, punctuation_file)\n",
    "    output_file = os.path.join(output_dir, punctuation_file.replace(\"punctuation.txt\", \"translation.txt\"))\n",
    "\n",
    "    try:\n",
    "        # Read input texts from the file\n",
    "        input_texts = []\n",
    "        current_input_text = \"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.strip().lower().startswith(\"outputs:\"):\n",
    "                    if current_input_text:\n",
    "                        input_texts.append(current_input_text.strip())\n",
    "                    current_input_text = \"\"\n",
    "                else:\n",
    "                    current_input_text += line\n",
    "\n",
    "            if current_input_text:\n",
    "                input_texts.append(current_input_text.strip())\n",
    "\n",
    "        # Check if any input texts were found\n",
    "        if not input_texts:\n",
    "            print(f\"No input texts found in {punctuation_file}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        src_lang, tgt_lang = \"kan_Knda\", \"eng_Latn\"\n",
    "\n",
    "        # Process input texts in batches\n",
    "        batch_size = 8  # Batch size for processing input texts\n",
    "        for i in range(0, len(input_texts), batch_size):\n",
    "            batch_input_texts = input_texts[i:i + batch_size]\n",
    "\n",
    "            # Preprocess the batch\n",
    "            batch = ip.preprocess_batch(batch_input_texts, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "\n",
    "            DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "            # Tokenize the sentences and generate input encodings\n",
    "            inputs = tokenizer(\n",
    "                batch,\n",
    "                truncation=True,\n",
    "                padding=\"longest\",\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True,\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            # Generate translations using the model\n",
    "            with torch.no_grad():\n",
    "                generated_tokens = model.generate(\n",
    "                    **inputs,\n",
    "                    use_cache=True,\n",
    "                    min_length=0,\n",
    "                    max_length=256,\n",
    "                    num_beams=5,\n",
    "                    num_return_sequences=1,\n",
    "                )\n",
    "\n",
    "            # Decode the generated tokens into text\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                generated_tokens = tokenizer.batch_decode(\n",
    "                    generated_tokens.detach().cpu().tolist(),\n",
    "                    skip_special_tokens=True,\n",
    "                    clean_up_tokenization_spaces=True,\n",
    "                )\n",
    "\n",
    "            # Postprocess the translations, including entity replacement\n",
    "            translations = ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
    "\n",
    "            # Write translations to output file\n",
    "            with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                for translation in translations:\n",
    "                    if not translation.startswith(f\"{src_lang}:\") and not translation.startswith(f\"{tgt_lang}:\"):\n",
    "                        f.write(f\"Output: {translation}\\n\")\n",
    "\n",
    "        print(f\"Translations saved to: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {punctuation_file}: {e}\")\n",
    "\n",
    "print(\"All translations are done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stZXqZRsCkUq"
   },
   "source": [
    "## English Punctuation Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLIaJiaZCkEg"
   },
   "outputs": [],
   "source": [
    "from om_transliterator import Transliterator\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Initialize the transliterator\n",
    "transliterator = Transliterator()\n",
    "\n",
    "# Define input directory path for punctuation files\n",
    "input_dir = \"./OUTPUT/TEXT/02_Punctuation\"\n",
    "\n",
    "# List all .txt files ending with punctuation.txt in the directory\n",
    "punctuation_files = [f for f in os.listdir(input_dir) if f.endswith(\"punctuation.txt\")]\n",
    "\n",
    "# Define output directory for transliteration files\n",
    "output_dir = \"./OUTPUT/TEXT/03_Transliteration\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def process_punctuation_file(punctuation_file):\n",
    "    input_file_path = os.path.join(input_dir, punctuation_file)\n",
    "    output_file_path = os.path.join(output_dir, punctuation_file.replace(\"punctuation.txt\", \"transliteration.txt\"))\n",
    "\n",
    "    # Read the original text from the file\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Find the line with \"Outputs:\" and read the subsequent lines\n",
    "    start_reading = True\n",
    "    original_text = \"\"\n",
    "    for line in lines:\n",
    "        if start_reading:\n",
    "            original_text += line.strip() + \" \"\n",
    "        # if \"Outputs:\" in line:\n",
    "        #     start_reading = True\n",
    "\n",
    "    # Perform transliteration\n",
    "    transliterated_text = transliterator.knda_to_latn(original_text.strip())\n",
    "\n",
    "    # Write the transliterated text to the output file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(transliterated_text)\n",
    "\n",
    "    print(f\"Transliteration results saved to: {output_file_path}\")\n",
    "\n",
    "# Use ThreadPoolExecutor to process files in parallel\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_punctuation_file, punctuation_file) for punctuation_file in punctuation_files]\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        future.result()  # Retrieve the result to catch any exceptions\n",
    "\n",
    "print(\"All transliterations are done.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
